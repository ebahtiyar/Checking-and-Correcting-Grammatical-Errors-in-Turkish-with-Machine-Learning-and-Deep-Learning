{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deda_model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO7OCsclbOkYskZ4UZ2ztiR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ebahtiyar/Checking-and-Correcting-Grammatical-Errors-in-Turkish-with-Machine-Learning-and-Deep-Learning/blob/main/deda_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8gAgO4tDqPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee1bfc59-1633-4ecc-d9f7-5de82dd56f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n",
            "Selecting previously unselected package google-drive-ocamlfuse.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../google-drive-ocamlfuse_0.7.27-0ubuntu1~ubuntu18.04.1_amd64.deb ...\n",
            "Unpacking google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Setting up google-drive-ocamlfuse (0.7.27-0ubuntu1~ubuntu18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "!mkdir -p drive\n",
        "!google-drive-ocamlfuse drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import sys\n",
        "sys.path.insert(0,\"/content/drive/My Drive/project\")\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import random as r\n",
        "import punc_preprocessing as punc\n",
        "import pickle\n",
        "import warnings\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "import functions as f\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from keras.models import load_model\n",
        "def replace_str_index(text,index=0,replacement=''):\n",
        "    return '%s%s%s'%(text[:index],replacement,text[index+1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WT79BWr8EWel",
        "outputId": "dc83e797-3779-4686-9df2-a6f13f5182d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dkvNMNS5kCk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_de_da_ki():\n",
        "    data = punc.load_data(\"/content/drive/My Drive/project/database.db\",\"Total_senteces_F1\")\n",
        "    c_data = []\n",
        "    for line in data:\n",
        "        words = line.split()\n",
        "        if len(words) < 35:\n",
        "          c_data.append(line)\n",
        "    da_de = []\n",
        "    ki = []\n",
        "    for i in c_data:\n",
        "        i = i.replace(\"’\",\"\")\n",
        "        i = re.sub(r'[^\\w\\s]', '', i)\n",
        "        try:\n",
        "            if i.find(\"da\") != -1 :\n",
        "                index  = i.find(\"da\")\n",
        "                if i[index + 2] == \" \" and i[index-1] != \"’\":\n",
        "                    da_de.append(i)\n",
        "            if i.find(\"de\") != -1:\n",
        "                index = i.find(\"de\")\n",
        "                if i[index + 2] == \" \" and i[index-1] != \"’\":\n",
        "                    da_de.append(i)\n",
        "            if i.find(\"ki\") != -1:\n",
        "                index = i.find(\"ki\")\n",
        "                if i[index + 2] == \" \" and i.find(\"iki\") == -1 and i.find(\"eski\") == -1:\n",
        "                    ki.append(i)\n",
        "                                      \n",
        "        except:\n",
        "            pass\n",
        "    return da_de,ki"
      ],
      "metadata": {
        "id": "Yniqsn89EYi9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v172ftxai6Tx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def data_simplify_conjunction():\n",
        "    da_de,ki = find_de_da_ki()\n",
        "    \n",
        "    sp_da_de = []\n",
        "    sp_ki = []\n",
        "    \n",
        "    adj_da_de = []\n",
        "    adj_ki = []\n",
        "    \n",
        "    \n",
        "    for i in da_de:\n",
        "        try:\n",
        "            if i.find(\"da\") != -1:\n",
        "                index = i.find(\"da\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_da_de.append(i)\n",
        "                else:\n",
        "                    adj_da_de.append(i)\n",
        "            if i.find(\"de\") != -1:\n",
        "                index = i.find(\"de\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_da_de.append(i)\n",
        "                else:\n",
        "                    adj_da_de.append(i)           \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    for i in ki:\n",
        "        try:\n",
        "            if i.find(\"ki\") != -1:\n",
        "                index = i.find(\"ki\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_ki.append(i)\n",
        "                else:\n",
        "                    adj_ki.append(i)         \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    return sp_da_de,sp_ki,adj_da_de,adj_ki  "
      ],
      "metadata": {
        "id": "eu1vB9huErq9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_training_input(data):\n",
        "    training_input = []\n",
        "    training_label = []\n",
        "    for line in data:\n",
        "        line = line.strip()\n",
        "        training_input.append(line)\n",
        "        r_words = word_tokenizer(line)\n",
        "        label = \"\"\n",
        "        for word in r_words:\n",
        "            label = label + \"O \"\n",
        "        training_label.append(label)\n",
        "        \n",
        "        b_da = [m.start() for m in re.finditer('da ', line)]\n",
        "        k  = 0\n",
        "        for pos in b_da:\n",
        "            if pos > 0:\n",
        "                pos = pos + k\n",
        "                if line[pos-1] == \" \":\n",
        "                    line = replace_str_index(line,index=pos-1,replacement='')\n",
        "                    k = k - 1\n",
        "                        \n",
        "                elif line[pos-1].isalpha():\n",
        "                    line = replace_str_index(line,index=pos,replacement=' d')\n",
        "                    k = k + 1\n",
        "                \n",
        "        b_de = [m.start() for m in re.finditer('de ', line)]\n",
        "        k  = 0     \n",
        "        for pos in b_de:\n",
        "            if pos > 0:\n",
        "                pos = pos + k\n",
        "                if line[pos-1] == \" \":\n",
        "                    line = replace_str_index(line,index=pos-1,replacement='')\n",
        "                    k = k - 1\n",
        "                        \n",
        "                elif line[pos-1].isalpha():\n",
        "                    line = replace_str_index(line,index=pos,replacement=' d')\n",
        "                    k = k + 1\n",
        "                    \n",
        "        training_input.append(line)\n",
        "        label1 = generator_training_output(line)\n",
        "        training_label.append(label1)\n",
        "        \n",
        "    return training_input,training_label"
      ],
      "metadata": {
        "id": "NYrNcA_BEtFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_training_output(line):\n",
        "    \n",
        "        label = \"\"\n",
        "        r_words = word_tokenizer(line)\n",
        "                    \n",
        "        for word in r_words:\n",
        "            if word.find(\"da\") > 0 or word.find(\"de\") > 0:\n",
        "                if len(word) > 1:\n",
        "                    if word[len(word) - 2] == \"d\":\n",
        "                        label = label + \"E \"\n",
        "                    else:\n",
        "                         label = label + \"O \"\n",
        "                else:\n",
        "                    label = label + \"O \"\n",
        "            else:\n",
        "                label = label + \"O \"\n",
        "        label = label.strip()\n",
        "        \n",
        "          \n",
        "        return label"
      ],
      "metadata": {
        "id": "yhm2IJp7EuZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def word_tokenizer(line):\n",
        "        line = re.sub(r'[^\\w\\s]', '', line)\n",
        "        words = line.split()\n",
        "        r_words = []\n",
        "        try:\n",
        "            for pos,word in enumerate(words):\n",
        "                n = \"\"\n",
        "                if pos > 0:\n",
        "                   if word == \"de\":\n",
        "                      before_word = words[pos-1]\n",
        "                      n = before_word + \" de\"\n",
        "                      r_words.remove(before_word)\n",
        "                      r_words.append(n)\n",
        "                   elif word == \"da\":\n",
        "                        before_word = words[pos-1]\n",
        "                        n = before_word + \" da\"\n",
        "                        r_words.remove(before_word)\n",
        "                        r_words.append(n)\n",
        "                           \n",
        "                   else:\n",
        "                       r_words.append(word)\n",
        "                else:\n",
        "                    r_words.append(word)\n",
        "        except:\n",
        "            pass\n",
        "                \n",
        "        return r_words"
      ],
      "metadata": {
        "id": "pkeza5B0E1N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createTokenizer(data,Idx):\n",
        "  output = []\n",
        "  for line in data:\n",
        "      wordIndices = []\n",
        "      words = word_tokenizer(line)\n",
        "      for word in words:\n",
        "        if word in Idx:\n",
        "          wordIdx = Idx[word].index\n",
        "        elif word.lower() in Idx:\n",
        "          wordIdx = Idx[word.lower()].index\n",
        "        else:\n",
        "          wordIdx = Idx['UNK'].index\n",
        "        wordIndices.append(wordIdx)\n",
        "      output.append(wordIndices)\n",
        "  return output"
      ],
      "metadata": {
        "id": "9U20L90kE4H8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def createTokenizer1(data,Idx):\n",
        "  output = []\n",
        "  for line in data:\n",
        "      wordIndices = []\n",
        "      words = word_tokenizer(line)\n",
        "      for word in words:\n",
        "        if word in Idx:\n",
        "          wordIdx = Idx[word]\n",
        "        elif word.lower() in Idx:\n",
        "          wordIdx = Idx[word.lower()]\n",
        "        else:\n",
        "          wordIdx = Idx['UNK']\n",
        "        wordIndices.append(wordIdx)\n",
        "      output.append(wordIndices)\n",
        "  return output"
      ],
      "metadata": {
        "id": "m9gAajk-V1N1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lastControl(pred_sentence):\n",
        "    word_list = []\n",
        "    r_words = word_tokenizer(pred_sentence)\n",
        "    sentence = \"\"\n",
        "    for pos,word in enumerate(r_words):\n",
        "        if word == \"dede\" or word == \"dada\":\n",
        "          if pos > 0:\n",
        "              if word == \"dede\":\n",
        "                before_word = r_words[pos-1]\n",
        "                new_word = before_word + \"de\" + \" de\"\n",
        "                word_list.append(new_word)\n",
        "                word_list.remove(before_word)\n",
        "              elif word == \"dada\":\n",
        "                  before_word = r_words[pos-1]\n",
        "                  new_word = before_word + \"da\" + \" da\"\n",
        "                  word_list.append(new_word)\n",
        "                  word_list.remove(before_word)\n",
        "              else:\n",
        "                word_list.append(word)\n",
        "          else:\n",
        "              word_list.append(word)\n",
        "        else:\n",
        "          word_list.append(word)\n",
        "    for word in word_list:\n",
        "        sentence = sentence + word + \" \"\n",
        "    return sentence.lower()"
      ],
      "metadata": {
        "id": "zfD10L7SxRKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_da_de,sp_ki,adj_da_de,adj_ki = data_simplify_conjunction()\n",
        "input_data = adj_da_de + sp_da_de\n",
        "input_data = f.unique(input_data)\n",
        "len(input_data)\n",
        "training_input,training_label = generator_training_input(input_data)\n",
        "print(\"Total Sentences: \" + str(len(training_input)))\n",
        "\n",
        "x_train , x_test , y_train , y_test = train_test_split(training_input,training_label,test_size=0.1)\n",
        "print(\"Total Training Sentences: \" + str(len(x_train)))"
      ],
      "metadata": {
        "id": "hbIq9xWcE-vT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8eb12cd6-ba57-45f8-c76e-529cd0235be3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sentences: 2081226\n",
            "Total Training Sentences: 1873103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "s = {\"Traininig Sentences\":training_input, \"Labels\": training_label }\n",
        "s_df = pd.DataFrame(data = s)"
      ],
      "metadata": {
        "id": "bwQkN29Qvufh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokenizer(sp_da_de[1121]) # 1141"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VljCIFPIzEOB",
        "outputId": "85877d33-89d3-4aaa-a281-e6e21f95933d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Belki de', 'bu', 'yüzden', 'filmlerim', 'insanlara', 'dokunuyor']"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s_df.sample(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "mPpNndTdwI3A",
        "outputId": "de552b7d-aa14-4f79-b1d2-83965774c14d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8a5de024-8b7e-424f-bfea-33fcd4f16e6c\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Traininig Sentences</th>\n",
              "      <th>Labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1276172</th>\n",
              "      <td>Müteahhitler arasında meydana gelecek sıkıntıl...</td>\n",
              "      <td>O O O O O O O O O O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>165029</th>\n",
              "      <td>Annemde babama aşık oluyor iki oğlu zaten var ...</td>\n",
              "      <td>E O O O O O O O E O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2066808</th>\n",
              "      <td>Şirkete döndük çalışıyoruz annemle de görüşüyoruz</td>\n",
              "      <td>O O O O O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1893081</th>\n",
              "      <td>Önümüzdeki haftalar da çok özel bir kütüphaned...</td>\n",
              "      <td>O E O O O E O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339931</th>\n",
              "      <td>Benim bulunduğum Beyoğlu ilçesin de TP plakası...</td>\n",
              "      <td>O O O E O O O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8a5de024-8b7e-424f-bfea-33fcd4f16e6c')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8a5de024-8b7e-424f-bfea-33fcd4f16e6c button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8a5de024-8b7e-424f-bfea-33fcd4f16e6c');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                       Traininig Sentences                        Labels\n",
              "1276172  Müteahhitler arasında meydana gelecek sıkıntıl...  O O O O O O O O O O O O O O \n",
              "165029   Annemde babama aşık oluyor iki oğlu zaten var ...       E O O O O O O O E O O O\n",
              "2066808  Şirkete döndük çalışıyoruz annemle de görüşüyoruz                    O O O O O \n",
              "1893081  Önümüzdeki haftalar da çok özel bir kütüphaned...                 O E O O O E O\n",
              "339931   Benim bulunduğum Beyoğlu ilçesin de TP plakası...                 O O O E O O O"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(input_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXN8IgfLuqb5",
        "outputId": "d1642705-506e-4ee6-9d46-a075c2b307cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1040613"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "j = sp_ki + adj_ki"
      ],
      "metadata": {
        "id": "xCLzD_Mvujx5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "227216*2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6XzhvgucwwPJ",
        "outputId": "1298541d-9746-407b-b6d7-6b213d381e52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "454432"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "j = f.unique(j)\n",
        "print(len(j))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh5bDyPbvIaA",
        "outputId": "36eabf6c-e517-46d5-aabf-d68a9cd303c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "227216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sp_ki[32]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "WsVWdIa-3OP6",
        "outputId": "3a788770-7b46-4f88-f94b-e46d26427897"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Ama gelin görün ki dünyanın adaleti böyle tecelli etmiyor'"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "traning_token =[]\n",
        "for i in training_input:\n",
        "  i = i.lower()\n",
        "  traning_token.append(word_tokenizer(i))\n",
        "traning_token.append([\"UNK\",\"PAD\"])"
      ],
      "metadata": {
        "id": "GKQbv2iuIm8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Word2Vec_model = Word2Vec(min_count=1)\n",
        "Word2Vec_model.build_vocab(traning_token) \n",
        "Word2Vec_model.train(traning_token, total_examples=Word2Vec_model.corpus_count, epochs= 100)"
      ],
      "metadata": {
        "id": "C0FqDrEQJc_E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f964d9d6-2adf-410f-bce9-b771f5c9763a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2320832462, 2466378200)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Word2Vec_model.save(\"/content/drive/MyDrive/project/savedd_model/Word2Vec_model\")"
      ],
      "metadata": {
        "id": "bmWSEqVFXRVa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_weights = Word2Vec_model.wv.syn0\n",
        "vocab_size, emdedding_size = pretrained_weights.shape"
      ],
      "metadata": {
        "id": "wVOcedPBSt6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7d7f59-9645-4885-c870-c27544b9bbed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `syn0` (Attribute will be removed in 4.0.0, use self.wv.vectors instead).\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_tokenizer = createTokenizer(x_train,Word2Vec_model.wv.vocab)\n",
        "\n",
        "labelSet = set()\n",
        "for line in y_train:\n",
        "  words = word_tokenizer(line)\n",
        "  for word in words:\n",
        "    labelSet.add(word.lower())\n",
        "\n",
        "sorted_labels = sorted(list(labelSet), key=len)\n",
        "# Create mapping for labels\n",
        "label2Idx = {}\n",
        "if len(label2Idx) == 0:\n",
        "  label2Idx[\"PAD\"] = len(label2Idx)\n",
        "  label2Idx[\"UNK\"] = len(label2Idx)\n",
        "for label in sorted_labels:\n",
        "  label2Idx[label] = len(label2Idx)\n",
        "idx2Label = {v: k for k, v in label2Idx.items()}\n",
        "\n",
        "y_train_tokenizer = createTokenizer1(y_train,label2Idx)"
      ],
      "metadata": {
        "id": "_bg6o1M-9OOt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_in_len = int(len(max(x_train_tokenizer,key=len)))\n",
        "max_out_len = int(len(max(x_train_tokenizer,key=len)))\n",
        "x_train_pad = pad_sequences(x_train_tokenizer, max_in_len, padding = \"post\")\n",
        "y_train_pad = pad_sequences(y_train_tokenizer, max_out_len, padding = \"post\")\n",
        "\n",
        "x_train_pad = x_train_pad.reshape(*x_train_pad.shape, 1)\n",
        "y_train_pad = y_train_pad.reshape(*y_train_pad.shape, 1)\n",
        "print(\"Max Length of Train Sentences:\" + str(max_in_len) + \" Max Length of Train Labels:\" + str(max_out_len))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Do9Hn-ncVU6N",
        "outputId": "7399023f-ad33-454c-9e2b-defb26e1c406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Max Length of Train Sentences:34 Max Length of Train Labels:34\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LWOmzFsvgC0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers.recurrent import LSTM\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Activation , Input,RepeatVector, TimeDistributed,Bidirectional\n",
        "from keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from keras.callbacks import ModelCheckpoint"
      ],
      "metadata": {
        "id": "yh4AmLuJWlF0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_main = Sequential()\n",
        "model_main.add(Input(shape=(max_in_len,)))\n",
        "model_main.add(Embedding(input_dim=vocab_size, output_dim=emdedding_size,weights=[pretrained_weights]))\n",
        "model_main.add(Bidirectional(LSTM(units=emdedding_size,return_sequences=False)))\n",
        "model_main.add(RepeatVector(max_out_len))\n",
        "model_main.add(Bidirectional(LSTM(emdedding_size, return_sequences=True, dropout=0.1)))\n",
        "model_main.add(TimeDistributed(Dense(len(label2Idx)+1)))\n",
        "model_main.add(Activation('softmax'))\n",
        "model_main.compile(loss=sparse_categorical_crossentropy,\n",
        "              optimizer=Adam(1e-3),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ZwxeTP8mWoc0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filepath = 'my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath=filepath, \n",
        "                             monitor='val_loss',\n",
        "                             verbose=1, \n",
        "                             save_best_only=True,\n",
        "                             mode='min')\n",
        "callbacks = [checkpoint]"
      ],
      "metadata": {
        "id": "7CUMEoxnXAFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_main.fit(x_train_pad, y_train_pad, validation_split = 0.2, epochs= 3 , callbacks=callbacks, batch_size = 256)"
      ],
      "metadata": {
        "id": "duCvF_juXCRN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_main.save('/content/drive/My Drive/project/savedd_model/model_dade_w2v_3.h5')#v_2 0.01077 best"
      ],
      "metadata": {
        "id": "Sw1Ec8O_i9Xb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_main  = load_model('/content/drive/My Drive/project/savedd_model/model_dade_w2v_3.h5')"
      ],
      "metadata": {
        "id": "6AcokASSIjQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx2Label = {0 :'PAD', 1:'UNK' , 2: 'o' , 3: 'e'}\n",
        "Word2Vec_model=Word2Vec.load(\"/content/drive/MyDrive/project/savedd_model/Word2Vec_model\")"
      ],
      "metadata": {
        "id": "OP_qDjq8fR1j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction(model_main,sample,Word2Vec_model,idx2Label):\n",
        "  sample_pad = []\n",
        "  words = word_tokenizer(sample.lower())\n",
        "  for word in words:\n",
        "      if word in Word2Vec_model.wv.vocab:\n",
        "          \n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[word].index)\n",
        "      else:\n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[\"UNK\"].index)\n",
        "\n",
        "  sample_pad = pad_sequences([sample_pad], maxlen=34, padding='post')\n",
        "\n",
        "  pred = model_main.predict(sample_pad)\n",
        "  label = [idx2Label[np.argmax(x)] for x in pred[0]]\n",
        "  output_label = \"\"\n",
        "  for i in label:\n",
        "      output_label = output_label + i + \" \"\n",
        "  \n",
        "\n",
        "  \n",
        "  pred_sent = \"\"\n",
        "  for i in zip(words,label):\n",
        "    if i[1] == \"o\":\n",
        "      pred_sent = pred_sent +  i[0] + \" \"\n",
        "    elif i[1] == \"e\":\n",
        "        if i[0].find(\" de\") > 0:\n",
        "            new_word = replace_str_index(i[0],index=i[0].find(\" de\"),replacement='')\n",
        "            pred_sent = pred_sent + new_word + \" \"\n",
        "        elif i[0].find(\" da\") > 0:\n",
        "            new_word = replace_str_index(i[0],index=i[0].find(\" da\"),replacement='')\n",
        "            pred_sent = pred_sent + new_word + \" \"\n",
        "        elif i[0].find(\" \") == -1 and i[0].find(\"da\") == len(i[0]) - 2:\n",
        "            new_word = replace_str_index(i[0],i[0].find(\"da\"),replacement=' d')\n",
        "            pred_sent = pred_sent + new_word + \" \"\n",
        "        elif i[0].find(\" \") == -1 and i[0].find(\"de\") == len(i[0]) - 2:\n",
        "            new_word = replace_str_index(i[0],i[0].find(\"de\"),replacement=' d')\n",
        "            pred_sent = pred_sent + new_word + \" \"\n",
        "        else:\n",
        "            pred_sent = pred_sent + i[0] + \" \"\n",
        "            \n",
        "    elif i[0][1] == \"PAD\":\n",
        "        continue\n",
        "  pred_sent = pred_sent.strip()\n",
        "\n",
        "  r_words = word_tokenizer(pred_sent)\n",
        "\n",
        "\n",
        "  return pred_sent"
      ],
      "metadata": {
        "id": "g-58OYtmjBjQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample = \"Ve meclislerin de artık her biri başkan olabilecek kadar yıldız isimlerden olması gerekir\""
      ],
      "metadata": {
        "id": "c1fhdfGYkL4o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction(model_main,sample.lower(),Word2Vec_model,idx2Label).strip()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "BV93n4R5kOwA",
        "outputId": "5a7f8e57-6441-4ac1-8053-f16edec5c639"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ve meclislerinde artık her biri başkan olabilecek kadar yıldız isimlerden olması gerekir'"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generator_testing_input(data):\n",
        "    training_input = []\n",
        "    training_label = []\n",
        "    for line in data:\n",
        "        line = line.strip()\n",
        "        b_da = [m.start() for m in re.finditer('da ', line)]\n",
        "        k  = 0\n",
        "        for pos in b_da:\n",
        "            if pos > 0:\n",
        "                pos = pos + k\n",
        "                if line[pos-1] == \" \":\n",
        "                    line = replace_str_index(line,index=pos-1,replacement='')\n",
        "                    k = k - 1\n",
        "                        \n",
        "                elif line[pos-1].isalpha():\n",
        "                    line = replace_str_index(line,index=pos,replacement=' d')\n",
        "                    k = k + 1\n",
        "                \n",
        "        b_de = [m.start() for m in re.finditer('de ', line)]\n",
        "        k  = 0     \n",
        "        for pos in b_de:\n",
        "            if pos > 0:\n",
        "                pos = pos + k\n",
        "                if line[pos-1] == \" \":\n",
        "                    line = replace_str_index(line,index=pos-1,replacement='')\n",
        "                    k = k - 1\n",
        "                        \n",
        "                elif line[pos-1].isalpha():\n",
        "                    line = replace_str_index(line,index=pos,replacement=' d')\n",
        "                    k = k + 1\n",
        "                    \n",
        "        training_input.append(line)\n",
        "    \n",
        "    return training_input"
      ],
      "metadata": {
        "id": "_e2HN6p-kbqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_de_da_ki1():\n",
        "    data = punc.load_data(\"/content/drive/My Drive/project/test.db\",\"test_sentences\")\n",
        "    c_data = []\n",
        "    for line in data:\n",
        "        words = line.split()\n",
        "        if len(words) < 35:\n",
        "          c_data.append(line)\n",
        "    da_de = []\n",
        "    ki = []\n",
        "    for i in c_data:\n",
        "        i = i.replace(\"’\",\"\")\n",
        "        i = re.sub(r'[^\\w\\s]', '', i)\n",
        "        try:\n",
        "            if i.find(\"da\") != -1 :\n",
        "                index  = i.find(\"da\")\n",
        "                if i[index + 2] == \" \" and i[index-1] != \"’\":\n",
        "                    da_de.append(i)\n",
        "            if i.find(\"de\") != -1:\n",
        "                index = i.find(\"de\")\n",
        "                if i[index + 2] == \" \" and i[index-1] != \"’\":\n",
        "                    da_de.append(i)\n",
        "            if i.find(\"ki\") != -1:\n",
        "                index = i.find(\"ki\")\n",
        "                if i[index + 2] == \" \" and i.find(\"iki\") == -1 and i.find(\"eski\") == -1:\n",
        "                    ki.append(i)\n",
        "                                      \n",
        "        except:\n",
        "            pass\n",
        "    return da_de,ki"
      ],
      "metadata": {
        "id": "WALkmzUu2GHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def data_simplify_conjunction1():\n",
        "    da_de,ki = find_de_da_ki1()\n",
        "    \n",
        "    sp_da_de = []\n",
        "    sp_ki = []\n",
        "    \n",
        "    adj_da_de = []\n",
        "    adj_ki = []\n",
        "    \n",
        "    \n",
        "    for i in da_de:\n",
        "        try:\n",
        "            if i.find(\"da\") != -1:\n",
        "                index = i.find(\"da\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_da_de.append(i)\n",
        "                else:\n",
        "                    adj_da_de.append(i)\n",
        "            if i.find(\"de\") != -1:\n",
        "                index = i.find(\"de\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_da_de.append(i)\n",
        "                else:\n",
        "                    adj_da_de.append(i)           \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    for i in ki:\n",
        "        try:\n",
        "            if i.find(\"ki\") != -1:\n",
        "                index = i.find(\"ki\")\n",
        "                if i[index-1] == \" \":\n",
        "                    sp_ki.append(i)\n",
        "                else:\n",
        "                    adj_ki.append(i)         \n",
        "        except:\n",
        "            pass\n",
        "        \n",
        "    return sp_da_de,sp_ki,adj_da_de,adj_ki  "
      ],
      "metadata": {
        "id": "rfKyMw9n2GP0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sp_da_de_t,sp_ki_t,adj_da_de_t,adj_ki_t  = data_simplify_conjunction1()"
      ],
      "metadata": {
        "id": "mR0HDUG-2Nxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(sp_da_de_t),len(adj_da_de_t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLd0Rxln2vU1",
        "outputId": "ebcd44c6-c924-40a5-bf38-81beddcef120"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(27735, 63345)"
            ]
          },
          "metadata": {},
          "execution_count": 138
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test = r.choices(sp_da_de_t,k = 100) + r.choices(adj_da_de_t , k = 100)\n",
        "test = f.unique(test)\n",
        "r.shuffle(test)\n",
        "testing_input = generator_testing_input(test)"
      ],
      "metadata": {
        "id": "T1YWCedSkcVp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVNlv03b2WRH",
        "outputId": "008c0322-f761-4d52-9ab3-5479ff165cdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10046"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred = []\n",
        "pred_last = []\n",
        "wrong = []\n",
        "wrong_pos = []\n",
        "for line in testing_input:\n",
        "    pred.append(prediction(model_main,line.lower(),Word2Vec_model,idx2Label).strip())\n",
        "\n",
        "for line in pred:\n",
        "    if (line.find(\" dede \") > 0) or (line.find(\" dada \") > 0):\n",
        "       line = lastControl(line)\n",
        "    pred_last.append(line.strip())\n",
        "   \n",
        " \n",
        "k = 0\n",
        "for pos,line in enumerate(test):\n",
        "    line = line.lower()\n",
        "    line = line.replace(\"i̇\",\"i\")\n",
        "    if line.lower() == pred_last[pos]:\n",
        "        k = k + 1\n",
        "    else:\n",
        "      wrong.append(pred_last[pos])\n",
        "      wrong_pos.append(pos)\n",
        "        "
      ],
      "metadata": {
        "id": "luatt6I9kvOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rand = r.randrange(100)\n",
        "print(\"Score: \" + str(k) +\" in \" + str(len(testing_input)) + \" sentences\")\n",
        "print(\"Orginal: \"+test[rand].lower())\n",
        "print(\"Wrong: \"+testing_input[rand].lower())\n",
        "print(\"Prediction: \"+pred[rand])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PqiK17Dk62Z",
        "outputId": "b827866d-fbfd-450d-89b1-1fc72229029d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score: 182 in 199 sentences\n",
            "Orginal: o poliklinik de çok başarılı oldu\n",
            "Wrong: o poliklinikde çok başarılı oldu\n",
            "Prediction: o poliklinik de çok başarılı oldu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prediction_score(model_main,sample,Word2Vec_model,idx2Label):\n",
        "  sample_pad = []\n",
        "  words = word_tokenizer(sample.lower())\n",
        "  for word in words:\n",
        "      if word in Word2Vec_model.wv.vocab:\n",
        "          \n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[word].index)\n",
        "      else:\n",
        "         sample_pad.append(Word2Vec_model.wv.vocab[\"UNK\"].index)\n",
        "\n",
        "  sample_pad = pad_sequences([sample_pad], maxlen=34, padding='post')\n",
        "\n",
        "  pred = model_main.predict(sample_pad)\n",
        "  label = [idx2Label[np.argmax(x)] for x in pred[0]]\n",
        "\n",
        "  return label"
      ],
      "metadata": {
        "id": "9Cd6d1V1o5rr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_label = []\n",
        "\n",
        "for line in testing_input:\n",
        "    pred_label.append(prediction_score(model_main,line.lower(),Word2Vec_model,idx2Label))\n",
        "\n",
        "\n",
        "pred_label_l = []\n",
        "\n",
        "for pred in pred_label:\n",
        "    t = []\n",
        "    for i in pred:\n",
        "        if i != \"PAD\":\n",
        "           if i == \"e\":\n",
        "              i = \"D_DEC\"\n",
        "           elif i == \"o\":\n",
        "              i = \"C_CON\"\n",
        "           t.append(i.upper())\n",
        "        else:\n",
        "          break\n",
        "    pred_label_l.append(t)"
      ],
      "metadata": {
        "id": "HPUdjGKAo5t_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_label = []\n",
        "\n",
        "for line in testing_input:\n",
        "    test_label.append(generator_training_output(line).lower())\n",
        "test = []\n",
        "for line in test_label:\n",
        "     line = line.upper()\n",
        "     x =  []\n",
        "     for l in line.split():\n",
        "         if l == \"O\":\n",
        "            i = \"D_CON\"\n",
        "         else:\n",
        "            i = \"D_DEC\"\n",
        "         x.append(i)\n",
        "     test.append(x)\n",
        "\n",
        "test_x = []\n",
        "text_label = []\n",
        "for i in range(len(test)):\n",
        "  if len(test[i]) == len(pred_label_l[i]):\n",
        "      test_x.append(test[i])\n",
        "      text_label.append(pred_label_l[i]) "
      ],
      "metadata": {
        "id": "VCzxBgdlo5wN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report, f1_score"
      ],
      "metadata": {
        "id": "Zdo0MqKI3HQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = classification_report(test_x,text_label)\n",
        "print(score)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-U8a9hHZpJ38",
        "outputId": "62d2c4d1-8e0c-4a4a-b1d4-e1ded236dc38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D_CON seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: D_DEC seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n",
            "/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py:171: UserWarning: C_CON seems not to be NE tag.\n",
            "  warnings.warn('{} seems not to be NE tag.'.format(chunk))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        _CON       0.84      0.83      0.83     22781\n",
            "        _DEC       0.87      0.87      0.87     14978\n",
            "\n",
            "   micro avg       0.85      0.85      0.85     37759\n",
            "   macro avg       0.86      0.85      0.85     37759\n",
            "weighted avg       0.85      0.85      0.85     37759\n",
            "\n"
          ]
        }
      ]
    }
  ]
}